import streamlit as st
import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from legal_splitter import LegalSemanticSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

# Sayfa yapÄ±landÄ±rmasÄ±
st.set_page_config(page_title="Legal-RAG: Kat MÃ¼lkiyeti AsistanÄ±", layout="wide")
st.title("ğŸ¢ Kat MÃ¼lkiyeti MevzuatÄ± AkÄ±llÄ± AsistanÄ±")

# Sabitler
DATA_PATH = "data/raw/kat-mulkiyeti-kanunu.pdf"
DB_DIR = "data/vector_db"

def initialize_rag():
    """RAG boru hattÄ±nÄ± ilklendirir."""
    
    # 1. PDF YÃ¼kleme
    if not os.path.exists(DATA_PATH):
        st.error(f"Hata: {DATA_PATH} bulunamadÄ±!")
        return None

    loader = PyPDFLoader(DATA_PATH)
    documents = loader.load()

    # 2. Ã–zel Hukuki Metin ParÃ§alama (Madde bazlÄ±)
    text_splitter = LegalSemanticSplitter(chunk_size=1500, chunk_overlap=200)
    splits = text_splitter.split_documents(documents)

    # 3. Embedding Modeli
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # 4. VektÃ¶r VeritabanÄ± (ChromaDB)
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        persist_directory=DB_DIR
    )
    vectorstore.persist()
    
    return vectorstore

# Sidebar - YapÄ±landÄ±rma ve Bilgi
with st.sidebar:
    st.header("Sistem Durumu")
    if st.button("VeritabanÄ±nÄ± Yeniden OluÅŸtur"):
        with st.spinner("Veriler iÅŸleniyor..."):
            st.session_state.vectorstore = initialize_rag()
            st.success("VeritabanÄ± gÃ¼ncellendi!")
    
    st.markdown("""
    ### HakkÄ±nda
    Bu asistan, Kat MÃ¼lkiyeti Kanunu Ã§erÃ§evesinde sorularÄ±nÄ±zÄ± yanÄ±tlar.
    
    **KullanÄ±lan Teknolojiler:**
    - LangChain
    - ChromaDB
    - Gemini Pro
    - Sentence Transformers
    """)

# RAG Kurulumu
if 'vectorstore' not in st.session_state:
    with st.spinner("Sistem hazÄ±rlanÄ±yor..."):
        st.session_state.vectorstore = initialize_rag()

# Soru-Cevap ArayÃ¼zÃ¼
if st.session_state.vectorstore:
    # LLM Kurulumu (Gemini)
    llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.3)

    # Prompt Åablonu
    template = """AÅŸaÄŸÄ±daki baÄŸlamÄ± (context) kullanarak kullanÄ±cÄ±nÄ±n sorusuna cevap ver. 
    EÄŸer cevabÄ± baÄŸlam iÃ§erisinde bulamÄ±yorsan, bilmediÄŸini sÃ¶yle, uydurma.
    CevabÄ±nÄ± her zaman ilgili kanun maddesine atÄ±fta bulunarak (Ã–rn: KMK Madde 12'ye gÃ¶re...) ver.

    BaÄŸlam:
    {context}

    Soru: {question}
    
    Cevap:"""
    
    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)

    # QA Chain
    qa_chain = RetrievalQA.from_chain_type(
        llm,
        retriever=st.session_state.vectorstore.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True,
        chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
    )

    # Chat ArayÃ¼zÃ¼
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Sorunuzu buraya yazÄ±n..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("DÃ¼ÅŸÃ¼nÃ¼yorum..."):
                result = qa_chain({"query": prompt})
                response = result["result"]
                sources = result["source_documents"]
                
                st.markdown(response)
                
                with st.expander("Kaynak DokÃ¼manlar"):
                    for i, doc in enumerate(sources):
                        st.write(f"**Kaynak {i+1}:**")
                        st.write(doc.page_content)
                        st.write("---")
                
                st.session_state.messages.append({"role": "assistant", "content": response})
else:
    st.warning("LÃ¼tfen veritabanÄ±nÄ± oluÅŸturmak iÃ§in PDF dosyasÄ±nÄ±n mevcut olduÄŸundan emin olun.")
